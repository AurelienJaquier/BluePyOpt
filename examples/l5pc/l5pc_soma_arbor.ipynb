{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Simulating optimized cells in Arbor and cross-validation with Neuron\n",
    "\n",
    "This notebook demonstrates how to run a simulation of a simple single compartmental cell with fixed/optimized parameters in Arbor. We follow the standard BluePyOpt flow of setting up an electrophysiological experiment and export the cell model to a mixed JSON/ACC-format. We then cross-validate voltage traces obtained with Arbor with those from a Neuron simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Choose subset of L5PC mechanisms to run (all regional mechs get re-mapped to soma)\n",
    "mechanism_defs = dict(\n",
    "    all=['pas'],\n",
    "    somatic=['hh'])\n",
    "\n",
    "extra_params = dict(\n",
    "    v_init='global',\n",
    "    # celsius='global',\n",
    "    cm=['all'],\n",
    "    Ra=['all'],\n",
    "    g_pas=['all'],  # add 'pas' to mechs on all above\n",
    "    e_pas=['all'],  # add 'pas' to mechs on all above\n",
    ")\n",
    "\n",
    "param_values_json = None\n",
    "\n",
    "default_dt = 0.025\n",
    "\n",
    "fine_dt = 0.001\n",
    "\n",
    "voltage_residual_rel_l1_tolerance = 5e-2\n",
    "\n",
    "run_spike_time_analysis = True\n",
    "\n",
    "run_fine_dt = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we need to compile the L5PC mechanisms and import the module that contains all the functionality to create electrical cell models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nrnivmodl mechanisms\n",
    "import bluepyopt as bpop\n",
    "import bluepyopt.ephys as ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from dataclasses import dataclass\n",
    "import typing\n",
    "import warnings\n",
    "import multiprocessing\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "import scipy.integrate\n",
    "import scipy.interpolate\n",
    "\n",
    "import arbor\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you want to see a lot of information about the internals, \n",
    "the verbose level can be set to 'debug' by commenting out\n",
    "the following lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setting up the cell model\n",
    "\n",
    "We use a single-compartimental cell model with the same morphology as in the `simplecell` example, but mechanisms from the `l5pc` model. They are instantiated with different options for axon replacement policy and (by default randomly sampled) mechanism parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameter randomization\n",
    "import json\n",
    "import random\n",
    "\n",
    "# os.chdir('../../../BluePyOpt/examples/l5pc')\n",
    "import l5pc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the protocols\n",
    "\n",
    "A protocol consists of a set of stimuli and recordings. These responses will later be used to compare voltage traces from simulations between Arbor and Neuron for different parameter values and axon replacement configurations.\n",
    "\n",
    "For the protocols, we apply stimuli centrally at the soma and probe the membrane voltage at a slightly displaced location. For this purpose, we introduce locations in Arbor, which are specified by a relative position `pos` on a `branch` of the morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arbor location\n",
    "class ArbLocation(ephys.locations.Location):\n",
    "    def __init__(self, name, branch, pos, comment=''):\n",
    "        super().__init__(_arb_label(name), comment)\n",
    "        self.branch = branch\n",
    "        self.pos = pos\n",
    "\n",
    "    def instantiate(self):\n",
    "        return _arb_unlabel(self.name), '(location %s %s)' % (self.branch, self.pos)\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def _arb_label(name):\n",
    "    return '\"%s\"' % name\n",
    "\n",
    "\n",
    "def _arb_unlabel(name):\n",
    "    return name[1:-1]\n",
    "\n",
    "\n",
    "# Make locations available to Arbor by instantiating them on the label dictionary \n",
    "def instantiate_locations(labels, locations):\n",
    "    labels.append(\n",
    "        arbor.label_dict(\n",
    "            dict([loc.instantiate() for loc in locations.values()])))\n",
    "\n",
    "\n",
    "# Define locations on branch 0 of the morphology (soma)\n",
    "arb_locations = dict(\n",
    "    stim_loc=ArbLocation(\n",
    "        name='stim_loc',\n",
    "        branch=0,\n",
    "        pos=0.5\n",
    "    ),\n",
    "    probe_loc=ArbLocation(\n",
    "        name='probe_loc',\n",
    "        branch=0,\n",
    "        pos=0.75\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nrn_locations = dict(\n",
    "    stim_loc=ephys.locations.NrnSeclistCompLocation(\n",
    "        name='soma',\n",
    "        seclist_name='somatic',\n",
    "        sec_index=0,\n",
    "        comp_x=0.5),\n",
    "    probe_loc=ephys.locations.NrnSeclistCompLocation(\n",
    "        name='probe',\n",
    "        seclist_name='somatic',\n",
    "        sec_index=0,\n",
    "        comp_x=0.75)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We use a modified version of the L5PC protocols with shortened stimulus duration and reduced amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protocol prots configuration\n",
    "protocol_steps = []\n",
    "for name, amplitude, duration in [('bAP', 0.19, 5), ('Step1', 0.0458, 50), ('Step3', 0.095, 50)]:\n",
    "    protocol_steps.append(\n",
    "        dict(name=name,\n",
    "             total_duration=120,\n",
    "             stimuli=[\n",
    "                 dict(name='%s.iclamp' % name,\n",
    "                      location='stim_loc',\n",
    "                      amplitude=amplitude,\n",
    "                      delay=40,\n",
    "                      duration=duration)],\n",
    "             recordings=[\n",
    "                 dict(name='%s.soma.v' % name,\n",
    "                      location='probe_loc')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current stimuli, voltage and spike recordings are made available to Arbor by lazily instantiating them on decors/cell models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current stimuli\n",
    "def instantiate_stimuli(decor, stimuli, locations):\n",
    "    for stim in stimuli:\n",
    "        decor.place(locations[stim['location']].name,\n",
    "                    arbor.iclamp(stim['delay'],\n",
    "                                 stim['duration'],\n",
    "                                 current=stim['amplitude']),\n",
    "                    stim['name'])\n",
    "\n",
    "# Spike detection with a voltage threshold of -10 mV\n",
    "# (different from spike_time observables in eFEL that measure 'peak_time')\n",
    "def instantiate_spike_recordings(decor, recordings, locations):\n",
    "    for i, rec in enumerate(recordings):\n",
    "        decor.place(locations[rec['location']].name,\n",
    "                    arbor.spike_detector(-10),\n",
    "                    'spike_detector' + '.%s' % i)\n",
    "\n",
    "# Attach voltage probe sampling at 10 kHz (every 0.1 ms).\n",
    "def instantiate_voltage_recordings(cell_model, recordings, locations):\n",
    "    for i, rec in enumerate(recordings):\n",
    "        # alternatively arbor.cable_probe_membrane_voltage\n",
    "        cell_model.probe('voltage',\n",
    "                         locations[rec['location']].name,\n",
    "                         frequency=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The protocols for Neuron are defined analogous to the L5PC model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sweep_protocols = []\n",
    "for prot_def in protocol_steps:\n",
    "    stims = []\n",
    "    for stim in prot_def['stimuli']:\n",
    "        stims.append(ephys.stimuli.NrnSquarePulse(step_amplitude=stim['amplitude'],\n",
    "                                                  step_delay=stim['delay'],\n",
    "                                                  step_duration=stim['duration'],\n",
    "                                                  location=nrn_locations[stim['location']],\n",
    "                                                  total_duration=prot_def['total_duration']))\n",
    "\n",
    "    recs = []\n",
    "    for rec in prot_def['recordings']:\n",
    "        recs.append(ephys.recordings.CompRecording(\n",
    "                    name=rec['name'],\n",
    "                    location=nrn_locations[rec['location']],\n",
    "                    variable='v'))\n",
    "\n",
    "    protocol = ephys.protocols.SweepProtocol(prot_def['name'], stims, recs)\n",
    "    sweep_protocols.append(protocol)\n",
    "\n",
    "nrn_protocol = ephys.protocols.SequenceProtocol('multistep', protocols=sweep_protocols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Running a protocol on an Arbor cable cell\n",
    "\n",
    "To run a protocol in Arbor, we need to export the cell model to a mixed JSON/ACC-format and assemble an Arbor cable cell on which we instantiate the locations, protocol stimuli and recordings. We use this cell to build a `single_cell_model` that sets up the constituents of an Arbor simulation and enables running a sweep protocol.\n",
    "\n",
    "To run the protocols also with Neuron, we follow the standard flow by creating a simulator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbSequenceProtocol(ephys.protocols.Protocol):\n",
    "    '''Run multiple sweep protocol steps and extract voltage traces/detected spikes'''\n",
    "\n",
    "    def __init__(self, name, protocols, locations):\n",
    "        super().__init__(name)\n",
    "        self.protocols = protocols\n",
    "        self.locations = locations\n",
    "\n",
    "    @staticmethod\n",
    "    def run_sweep_protocol(prot_def, locations, cell, params, dt):\n",
    "        '''Write cell model to ACC/JSON and run sweep protocol step'''\n",
    "        # Export cell model to mixed JSON/ACC-format\n",
    "        with tempfile.TemporaryDirectory() as acc_dir:\n",
    "            ephys.create_acc.output_acc(acc_dir, cell, params)\n",
    "            cell_json, morph, labels, decor = \\\n",
    "                ephys.create_acc.read_acc(\n",
    "                    os.path.join(acc_dir, cell.name + '.json'))\n",
    "\n",
    "        # Instantiate protocols on cable cell components\n",
    "        instantiate_locations(labels, locations)\n",
    "        instantiate_stimuli(decor, prot_def['stimuli'], locations)\n",
    "        instantiate_spike_recordings(decor, prot_def['recordings'], locations)\n",
    "        \n",
    "        # Create cable cell\n",
    "        cable_cell = arbor.cable_cell(morph, labels, decor)\n",
    "        # can output and visualize the cable cell in the Arbor GUI using\n",
    "        # arbor.write_component(cable_cell, '<filename>.acc')\n",
    "\n",
    "        # Create single cell model\n",
    "        arb_cell_model = arbor.single_cell_model(cable_cell)\n",
    "\n",
    "        # Add catalogues with explicit qualifiers\n",
    "        arb_cell_model.properties.catalogue = arbor.catalogue()\n",
    "        arb_cell_model.properties.catalogue.extend(\n",
    "            arbor.default_catalogue(), \"default::\")\n",
    "        arb_cell_model.properties.catalogue.extend(\n",
    "            arbor.bbp_catalogue(), \"BBP::\")\n",
    "    \n",
    "        # Instantiate remaining voltage recording\n",
    "        instantiate_voltage_recordings(arb_cell_model, prot_def['recordings'], locations)\n",
    "\n",
    "        # Run the simulation for the protocol step\n",
    "        arb_cell_model.run(tfinal=prot_def['total_duration'], dt=dt)\n",
    "\n",
    "        # Collect results\n",
    "        arb_resp = dict()\n",
    "        for i, rec in enumerate(prot_def['recordings']):\n",
    "            arb_resp[rec['name']] = \\\n",
    "                dict(time=arb_cell_model.traces[i].time,\n",
    "                     voltage=arb_cell_model.traces[i].value,\n",
    "                     spikes=arb_cell_model.spikes)  # TODO: handle global callback\n",
    "\n",
    "        return arb_resp\n",
    "\n",
    "\n",
    "    def run(self, cell_model, params, dt):\n",
    "        arb_resp = dict()\n",
    "        for prot_def in self.protocols:\n",
    "            arb_resp.update(self.run_sweep_protocol(\n",
    "                prot_def, self.locations, cell_model, params, dt))\n",
    "        return arb_resp\n",
    "\n",
    "\n",
    "arb_protocol = ArbSequenceProtocol('multistep', protocol_steps, arb_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation of Arbor and Neuron voltage traces\n",
    "\n",
    "To validate Arbor's simulation output with that of Neuron, we run the protocols over a set of parameter values, either loaded from a JSON-file or randomly sampled from the parameter bounds (using `random.uniform(*bounds)`) and both with and without axon replacement.\n",
    "\n",
    "First, we gather L5PC mechanisms and parameters according to `mechanism_defs` and `extra_params` in the top cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mechanisms = l5pc_model.create_mechanisms(\n",
    "    dict(all=mechanism_defs['all'],\n",
    "         somatic=[mech for loc, mechs in mechanism_defs.items() \n",
    "                  if loc != 'all' for mech in mechs]))\n",
    "\n",
    "l5pc_param_configs = l5pc_model.load_parameters()\n",
    "l5pc_param_names = []\n",
    "for p in l5pc_param_configs:\n",
    "    if 'mech' in p and p['mech'] in mechanism_defs.get(p['sectionlist'], []):\n",
    "        l5pc_param_names.append(p['param_name'] + '.' + p['sectionlist'])\n",
    "\n",
    "param_configs = []\n",
    "for p in l5pc_param_configs:\n",
    "    if 'mech' not in p:\n",
    "        if p['param_name'] in extra_params and (p['type'] == 'global' or \\\n",
    "            p['sectionlist'] in extra_params[p['param_name']]):\n",
    "            if p['type'] != 'global' and p['sectionlist'] != 'all':\n",
    "                p['sectionlist'] = 'somatic'  # remap to soma\n",
    "            param_configs.append(p)\n",
    "\n",
    "    elif p['mech'] in mechanism_defs.get(p['sectionlist'], []):\n",
    "        p['sectionlist'] = 'somatic'  # remap to soma\n",
    "        param_configs.append(p)\n",
    "\n",
    "if 'somatic' in mechanism_defs and 'hh' in mechanism_defs['somatic']:\n",
    "    for param_name, bounds in [('gnabar', (0.05, 0.125)),\n",
    "                               ('gkbar', (0.01, 0.075))]:\n",
    "        param_configs.append({\n",
    "            \"param_name\": param_name + \"_hh\",\n",
    "            \"mech\": \"hh\",\n",
    "            \"bounds\": bounds,\n",
    "            \"dist_type\": \"uniform\",\n",
    "            \"type\": \"range\",\n",
    "            \"sectionlist\": \"somatic\"\n",
    "        })\n",
    "\n",
    "parameters = l5pc_model.create_parameters(param_configs)\n",
    "\n",
    "# Print the names of all parameters\n",
    "[p.name for p in parameters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We either randomly sample or read the values of non-frozen parameters from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hasattr(arbor, \"prune_tag\"):  # skip axon replacement if not yet supported\n",
    "    replace_axon = [False]\n",
    "else:\n",
    "    replace_axon = [False, True]\n",
    "\n",
    "non_frozen_parameters = [param for param in parameters if not param.frozen]\n",
    "param_names = [param.name for param in non_frozen_parameters]\n",
    "\n",
    "if param_values_json is not None:\n",
    "    with open(param_values_json) as f:\n",
    "        params = []\n",
    "        for param_sample in json.load(f):\n",
    "            ps = dict()\n",
    "            for p_name, p_value in param_sample.items():\n",
    "                if p_name in l5pc_param_names:\n",
    "                    ps[p_name.split('.')[0] + '.somatic'] = p_value\n",
    "            params.append(ps)\n",
    "else:\n",
    "    params = [{param.name: random.uniform(*param.bounds)\n",
    "               for param in non_frozen_parameters}\n",
    "               for i in range(10)]\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We use a factory to create cell models with different configurations. This enables us to run both Arbor and Neuron simulations for the defined sequence protocols and each combination of axon replacement and parameter values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CellModelFactory:\n",
    "    mechanisms: typing.List[ephys.mechanisms.NrnMODMechanism]\n",
    "    parameters: typing.List[ephys.parameters.NrnParameter]\n",
    "\n",
    "    def create_cell_model(self, do_replace_axon):\n",
    "\n",
    "        morphology = ephys.morphologies.NrnFileMorphology(\n",
    "            '../simplecell/simple.swc', do_replace_axon=do_replace_axon)\n",
    "\n",
    "        return ephys.models.CellModel(\n",
    "            'simple_cell',\n",
    "            morph=morphology,\n",
    "            mechs=self.mechanisms,\n",
    "            params=self.parameters)\n",
    "\n",
    "@dataclass\n",
    "class SimulationRunner:\n",
    "    cell_factory: CellModelFactory\n",
    "    arb_protocol: ArbSequenceProtocol\n",
    "    nrn_protocol: ephys.protocols.SequenceProtocol\n",
    "\n",
    "    def run_all(self, replace_axon_policies, param_list, dt=0.025):\n",
    "        arb_resp = dict()\n",
    "        nrn_resp = dict()\n",
    "\n",
    "        nrn_sim = ephys.simulators.NrnSimulator(dt=dt)\n",
    "\n",
    "        for do_replace_axon in replace_axon_policies:\n",
    "            for param_i in range(len(param_list)):\n",
    "\n",
    "                cell_model = self.cell_factory.create_cell_model(do_replace_axon=do_replace_axon)\n",
    "\n",
    "                # calculate morphology with axon-replacement in Neuron\n",
    "                cell_model.instantiate_morphology(nrn_sim)\n",
    "\n",
    "                key = (do_replace_axon, param_i)\n",
    "                arb_resp[key] = self.arb_protocol.run(cell_model, param_list[param_i], dt=dt)\n",
    "\n",
    "                # need to destroy instantiated cell model first to avoid Hoc serialization error\n",
    "                cell_model.destroy(sim=nrn_sim)\n",
    "\n",
    "                nrn_resp[key] = self.nrn_protocol.run(cell_model, param_list[param_i], nrn_sim)\n",
    "        return arb_resp, nrn_resp\n",
    "\n",
    "\n",
    "cell_factory = CellModelFactory(mechanisms, parameters)\n",
    "simulation_runner = SimulationRunner(cell_factory, arb_protocol, nrn_protocol)\n",
    "arb_responses, nrn_responses = simulation_runner.run_all(replace_axon, params, default_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and to plot the responses for visual validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_response_comparison(arb_resp, nrn_resp, title):\n",
    "    num_protocols = len(protocol_steps)\n",
    "    num_recs = max([len(step['recordings']) for step in protocol_steps])\n",
    "    assert num_recs == 1  # add j as a second index with multiple recordings\n",
    "    fig, ax = plt.subplots(num_protocols, num_recs, figsize=(12,7))\n",
    "    for i, step in enumerate(protocol_steps):\n",
    "        for j, rec in enumerate(step['recordings']):\n",
    "            rec_name = rec['name']\n",
    "            ax[i].plot(nrn_resp[rec_name]['time'], nrn_resp[rec_name]['voltage'], label='Neuron ' + rec_name)\n",
    "            ax[i].plot(arb_resp[rec_name]['time'], arb_resp[rec_name]['voltage'], label='Arbor ' + rec_name)\n",
    "            ax[i].set_title(title)\n",
    "            ax[i].legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the responses, we compare the difference of voltage traces between Arbor and Neuron in the L1-norm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def voltage_trace_residual_l1_norm(int_resp, ref_resp):\n",
    "    int_resp_func = scipy.interpolate.interp1d(int_resp['time'], int_resp['voltage'], kind='cubic')\n",
    "    ref_resp_func = scipy.interpolate.interp1d(ref_resp['time'], ref_resp['voltage'], kind='cubic')\n",
    "    abs_diff = lambda t: abs(int_resp_func(t)-ref_resp_func(t))\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=scipy.integrate.IntegrationWarning)\n",
    "        return scipy.integrate.quad(abs_diff, ref_resp['time'][0], ref_resp['time'][-1], limit=400)\n",
    "\n",
    "\n",
    "def voltage_trace_residual_l1_norm_sweep_protocol(args):\n",
    "    arb_resp = args['arb_resp']\n",
    "    nrn_resp = args['nrn_resp']\n",
    "\n",
    "    residual_l1_norm, residual_error = voltage_trace_residual_l1_norm(\n",
    "        nrn_resp, arb_resp)\n",
    "\n",
    "    nrn_to_min_l1_norm, nrn_to_min_error = \\\n",
    "        voltage_trace_residual_l1_norm(\n",
    "            nrn_resp,\n",
    "            dict(time=nrn_resp['time'].values,\n",
    "                 voltage=numpy.full(nrn_resp['voltage'].shape,\n",
    "                                    numpy.min(nrn_resp['voltage']))))\n",
    "    return dict(\n",
    "        residual_rel_l1_norm=residual_l1_norm/nrn_to_min_l1_norm,\n",
    "        residual_rel_l1_error=residual_error/residual_l1_norm + \\\n",
    "                            nrn_to_min_error/nrn_to_min_l1_norm\n",
    "    )\n",
    "\n",
    "\n",
    "def analyze_voltage_traces_l1(arb_resp, nrn_resp):\n",
    "    indices = [(key,step) for key in arb_resp for step in arb_resp[key]]\n",
    "\n",
    "    # test_l5pc: insert if True:\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        l1_task_results = pool.map(voltage_trace_residual_l1_norm_sweep_protocol,\n",
    "                                   [dict(arb_resp=arb_resp[key][step],\n",
    "                                         nrn_resp=nrn_resp[key][step])\n",
    "                                    for key, step in indices])\n",
    "\n",
    "    l1_results = []\n",
    "    for l1_task_result, (key, step) in zip(l1_task_results, indices):\n",
    "            l1_results.append(\n",
    "                dict(replace_axon=key[0],\n",
    "                     param_i=key[1],\n",
    "                     **params[key[1]],\n",
    "                     protocol=step,\n",
    "                     **l1_task_result))\n",
    "\n",
    "    return pandas.DataFrame(l1_results)\n",
    "\n",
    "\n",
    "l1_results = analyze_voltage_traces_l1(arb_responses, nrn_responses)\n",
    "pandas.options.display.float_format = '{:,.3g}'.format\n",
    "l1_results.sort_values(by='residual_rel_l1_norm', ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_voltage_trace_l1_results(config_str, l1_results):\n",
    "    voltage_residual_rel_l1_error_tolerance = 1e-2 * voltage_residual_rel_l1_tolerance\n",
    "\n",
    "    mean_rel_l1_norm = l1_results['residual_rel_l1_norm'].mean()\n",
    "    mean_rel_l1_error = l1_results['residual_rel_l1_error'].mean()\n",
    "    # max_l1_norm_record = l1_results.loc[l1_results['residual_rel_l1_norm'].idxmax()]\n",
    "\n",
    "    message = '{}: test_l5pc %s! The mean relative Arbor-Neuron L1-deviation and error (tol in brackets) are {:,.3g} ({:,.3g}), {:,.3g} ({:,.3g}).'.format(\n",
    "        config_str, mean_rel_l1_norm, voltage_residual_rel_l1_tolerance, mean_rel_l1_error, voltage_residual_rel_l1_error_tolerance)\n",
    "    if mean_rel_l1_norm < voltage_residual_rel_l1_tolerance and mean_rel_l1_error < voltage_residual_rel_l1_error_tolerance:\n",
    "        print(message % 'OK')\n",
    "    else:\n",
    "        print(message % 'ERROR')\n",
    "\n",
    "\n",
    "print_voltage_trace_l1_results('Default dt ({:,.3g})'.format(default_dt), l1_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(arb_resp, nrn_resp, l1_results, *key):\n",
    "    if key in arb_resp:\n",
    "        plot_response_comparison(arb_resp[key], nrn_resp[key], 'replace_axon = %s, param_i = %s ' % (key[0], key[1]))\n",
    "        display(l1_results[(l1_results['replace_axon'] == key[0]) & (l1_results['param_i'] == key[1])])\n",
    "\n",
    "\n",
    "for param_i in range(len(params)):  # test_l5pc: skip\n",
    "    for do_replace_axon in replace_axon:  # test_l5pc: skip\n",
    "        compare_responses(arb_responses, nrn_responses, l1_results, do_replace_axon, param_i)  # test_l5pc: skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voltage traces look mostly similar between Arbor and Neuron. Under certain conditions, we can perform spike time analysis to understand this quantitatively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spike time cross-validation\n",
    "\n",
    "To compare Arbor and Neuron voltage traces further, we analyze the spike counts and times with the eFEL library and Arbor's built-in spike detector. Note that while eFEL measures the `peak_time`, Arbor's spike detector as configured above will measure the time when the voltage passes a threshold of -10 mV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis:\n",
    "\n",
    "    efel_features = ['Spikecount',\n",
    "                    'time_to_first_spike',\n",
    "                    'time_to_second_spike',\n",
    "                    'time_to_last_spike']\n",
    "\n",
    "\n",
    "    # Extract spike observables from protocol simulation responses\n",
    "    def get_spike_data(protocols, do_replace_axon, param_values,\n",
    "                       arb_resp, nrn_resp):\n",
    "        spike_res = []\n",
    "\n",
    "        for step in protocols:\n",
    "            recording_name = step['recordings'][0]['name'] # use only first recording\n",
    "            stim_start = min([stim['delay'] for stim in step['stimuli']])\n",
    "            stim_end = max([stim['delay'] + stim['duration'] for stim in step['stimuli']])\n",
    "\n",
    "            for efel_feature_name in efel_features:\n",
    "                # Calculate spike observables with eFEL\n",
    "                feature_name = '%s.%s' % (step['name'], efel_feature_name)\n",
    "                feature = ephys.efeatures.eFELFeature(\n",
    "                            feature_name,\n",
    "                            efel_feature_name=efel_feature_name,\n",
    "                            recording_names={'': recording_name},\n",
    "                            stim_start=stim_start,\n",
    "                            stim_end=stim_end)\n",
    "\n",
    "                # Calculate spike observables with Arbor\n",
    "                try:\n",
    "                    if efel_feature_name == 'Spikecount':\n",
    "                        arbor_int = len(arb_resp[recording_name]['spikes'])\n",
    "                    elif efel_feature_name == 'time_to_first_spike':\n",
    "                        arbor_int = arb_resp[recording_name]['spikes'][0]-stim_start\n",
    "                    elif efel_feature_name == 'time_to_second_spike':\n",
    "                        arbor_int = arb_resp[recording_name]['spikes'][1]-stim_start\n",
    "                    elif efel_feature_name == 'time_to_last_spike':\n",
    "                        arbor_int = arb_resp[recording_name]['spikes'][-1]-stim_start\n",
    "                except Exception:\n",
    "                    arbor_int = numpy.nan\n",
    "\n",
    "                spike_res.append(dict(\n",
    "                    replace_axon=do_replace_axon,\n",
    "                    protocol=step['name'],\n",
    "                    **param_values,\n",
    "                    efel=efel_feature_name,\n",
    "                    Neuron=feature.calculate_feature(nrn_resp),\n",
    "                    Arbor=feature.calculate_feature(arb_resp),\n",
    "                    Arbor_int=arbor_int))\n",
    "        return spike_res\n",
    "\n",
    "\n",
    "    # Compare spike observables between Arbor and Neuron\n",
    "    def analyze_spikes(spike_res):\n",
    "        spike_res_df = pandas.DataFrame(spike_res)\n",
    "        spike_res_df.set_index(\n",
    "            ['replace_axon', 'protocol',\n",
    "             *param_names, 'efel'], inplace=True)\n",
    "        spike_res_df.dropna(how='all', inplace=True)  # drop all-NaN rows\n",
    "\n",
    "        # Arbor to Neuron cross-validation with eFEL\n",
    "        spike_res_df['abs_diff Arbor to Neuron'] = \\\n",
    "            spike_res_df.apply(\n",
    "                lambda r: abs(r['Arbor']-r['Neuron']), axis=1)\n",
    "        spike_res_df['rel_abs_diff Arbor to Neuron [%]'] = \\\n",
    "            spike_res_df.apply(\n",
    "                lambda r: 100.*abs(r['Arbor']-r['Neuron'])/r['Neuron']\n",
    "                          if r['Neuron'] != 0 else numpy.nan, axis=1)\n",
    "\n",
    "        # Cross-validation of eFEL's spike detection with Arbor's\n",
    "        spike_res_df['abs_diff eFEL to Arbor-internal'] = \\\n",
    "            spike_res_df.apply(\n",
    "                lambda r: abs(r['Arbor']-r['Arbor_int']), axis=1)\n",
    "        spike_res_df['rel_abs_diff eFEL to Arbor-internal [%]'] = \\\n",
    "            spike_res_df.apply(\n",
    "                lambda r: 100.*abs(r['Arbor']-r['Arbor_int'])/r['Arbor_int']\n",
    "                          if r['Arbor_int'] != 0 else numpy.nan, axis=1)\n",
    "        return spike_res_df\n",
    "\n",
    "\n",
    "    # Aggregate all simulations into a single data frame \n",
    "    def joint_spike_analysis(arb_resp, nrn_resp, replace_axon_policies, param_list):\n",
    "        return pandas.concat(\n",
    "            [analyze_spikes(get_spike_data(protocol_steps,\n",
    "                                           replace_axon_policies[key[0]],\n",
    "                                           param_list[key[1]],\n",
    "                                           arb_resp[key],\n",
    "                                           nrn_resp[key]))\n",
    "             for key in arb_resp], axis=0)\n",
    "\n",
    "\n",
    "    pandas.options.display.float_format = '{:,.3g}'.format\n",
    "    # pandas.options.display.max_rows = None  # uncomment for full view\n",
    "    spike_results = joint_spike_analysis(arb_responses, nrn_responses, replace_axon, params)\n",
    "    display(spike_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the deviations over the entire parameter set and different axon replacement policies, we explore the per eFEL-observable statistics. Compare the `Spikecount`s that are usually fully consistent between Arbor and Neuron, whereas `time_to_last_spike` are the least consistent of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis:\n",
    "    display(spike_results[['abs_diff Arbor to Neuron',\n",
    "                           'rel_abs_diff Arbor to Neuron [%]']].groupby('efel').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the traces with highest difference in `time_to_last_spike` to identify outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis:\n",
    "    display(spike_results[ [el[spike_results.index.names.index('efel')] == 'time_to_last_spike'\n",
    "                            for el in spike_results.index] ].sort_values(\n",
    "                            by='abs_diff Arbor to Neuron', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the spike times, we find the anticipated bias between eFEL and Arbor's internal spike detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis:\n",
    "    display(spike_results[['abs_diff eFEL to Arbor-internal',\n",
    "                           'rel_abs_diff eFEL to Arbor-internal [%]']].groupby('efel').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running protocols with a finer time step\n",
    "\n",
    "To rule out the discretization as a possible source of the above error in `time_to_last_spike`, we can re-run the simulations at a smaller `dt` of 0.001 ms (default is 0.025 ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_fine_dt:\n",
    "    arb_responses_fine_dt, nrn_responses_fine_dt = simulation_runner.run_all(replace_axon, params, dt=fine_dt)\n",
    "\n",
    "    l1_results_fine_dt = analyze_voltage_traces_l1(arb_responses_fine_dt, nrn_responses_fine_dt)\n",
    "\n",
    "    display(l1_results_fine_dt.sort_values(by='residual_rel_l1_norm', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_fine_dt:\n",
    "    print_voltage_trace_l1_results('Fine dt ({:,.3g})'.format(fine_dt), l1_results_fine_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_fine_dt:\n",
    "    for param_i in range(len(params)):\n",
    "        for do_replace_axon in replace_axon:\n",
    "            compare_responses(arb_responses_fine_dt,\n",
    "                              nrn_responses_fine_dt,\n",
    "                              l1_results_fine_dt,\n",
    "                              do_replace_axon, param_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis and run_fine_dt:\n",
    "    spike_results_fine_dt = joint_spike_analysis(arb_responses_fine_dt, nrn_responses_fine_dt, replace_axon, params)\n",
    "    display(spike_results_fine_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis and run_fine_dt:\n",
    "    display(spike_results_fine_dt[['abs_diff Arbor to Neuron',\n",
    "                                   'rel_abs_diff Arbor to Neuron [%]']].groupby('efel').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier in `time_to_last_spike` is gone now, both visually and quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis and run_fine_dt:\n",
    "    display(spike_results_fine_dt[ [el[spike_results_fine_dt.index.names.index('efel')] == 'time_to_last_spike'\n",
    "                                    for el in spike_results_fine_dt.index] ].sort_values(\n",
    "                                    by='abs_diff Arbor to Neuron', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis and run_fine_dt:\n",
    "    display(spike_results_fine_dt[['abs_diff eFEL to Arbor-internal',\n",
    "                                   'rel_abs_diff eFEL to Arbor-internal [%]']].groupby('efel').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the mean deviation between Arbor and Neuron for eFEL spike times is significantly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_spike_time_analysis and run_fine_dt:\n",
    "    display((spike_results_fine_dt[['abs_diff Arbor to Neuron']].groupby('efel').mean()/\n",
    "             spike_results[['abs_diff Arbor to Neuron']].groupby('efel').mean()).rename(\n",
    "                 columns={'abs_diff Arbor to Neuron': \n",
    "                         'ratio of mean abs_diff Arbor to Neuron for fine dt vs. default dt'}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "581988038cf9ce8838e7faf3da7c29f4ff88d898cd43cb17e0086e389d8deda2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
